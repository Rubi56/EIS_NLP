import arxiv
import fitz  # PyMuPDF
import pandas as pd
import streamlit as st
import os
import re
import sqlite3
from datetime import datetime
import logging
import tempfile
import requests
import zipfile
from pathlib import Path
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np
import io

# ==============================
# ENVIRONMENT & PATH SETUP
# ==============================
def is_streamlit_cloud():
    """Detect if running on Streamlit Cloud."""
    return (
        os.getenv("HOME") == "/home/appuser" or
        "streamlitapp.com" in os.getenv("HOSTNAME", "") or
        os.getenv("IS_STREAMLIT_CLOUD", "false").lower() == "true"
    )

IS_CLOUD = is_streamlit_cloud()
BASE_DIR = Path(tempfile.gettempdir())
METADATA_DB_FILE = BASE_DIR / "eis_ml_metadata.db"
UNIVERSE_DB_FILE = BASE_DIR / "eis_ml_universe.db"
LOG_FILE = BASE_DIR / "eis_ml_query.log"

BASE_DIR.mkdir(exist_ok=True)

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ==============================
# STREAMLIT PAGE CONFIGURATION
# ==============================
st.set_page_config(
    page_title="EIS & ML Knowledge Explorer",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("üî¨ EIS & ML Context-Aware Knowledge Explorer")
st.markdown("""
This advanced tool queries **arXiv** for literature on **Universal Modeling of EIS Data through NLP and ML**, focusing on:
- **Universal Modeling** (Cross-chemistry and cross-condition generalization)
- **Context Extraction** (NLP-based extraction of Temp, SoC, and Chemistry metadata)
- **Equivalent Circuit Models (ECM)** & **Distribution of Relaxation Times (DRT)**
- **Health Prognostics** (SOH, RUL, and internal resistance prediction)
- **Advanced ML** (CNNs, Transformers, and Physics-Informed Neural Networks for EIS)

It uses **SciBERT with attention-aware relevance scoring** (>30% threshold) and stores:
- **Metadata** in `eis_ml_metadata.db`
- **Full extracted text** in `eis_ml_universe.db`
""")

if IS_CLOUD:
    st.info("‚òÅÔ∏è **Streamlit Cloud Mode**: All files are stored temporarily in `/tmp`. Use download buttons before your session expires!")

# ==============================
# SESSION STATE INITIALIZATION
# ==============================
if "log_buffer" not in st.session_state:
    st.session_state.log_buffer = []
if "downloaded_pdfs" not in st.session_state:
    st.session_state.downloaded_pdfs = {}
if "universe_db_updated" not in st.session_state:
    st.session_state.universe_db_updated = False
if "papers_df" not in st.session_state:
    st.session_state.papers_df = None
if "search_performed" not in st.session_state:
    st.session_state.search_performed = False

def update_log(message):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    st.session_state.log_buffer.append(f"[{timestamp}] {message}")
    if len(st.session_state.log_buffer) > 50:
        st.session_state.log_buffer.pop(0)
    logging.info(message)

# ==============================
# SCIBERT MODEL LOADING
# ==============================
@st.cache_resource
def load_scibert():
    update_log("Loading SciBERT model and tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained("allenai/scibert_scivocab_uncased")
        model = AutoModel.from_pretrained("allenai/scibert_scivocab_uncased")
        model.eval()
        update_log("SciBERT loaded successfully.")
        return tokenizer, model
    except Exception as e:
        update_log(f"Failed to load SciBERT: {e}")
        raise e

try:
    scibert_tokenizer, scibert_model = load_scibert()
except Exception as e:
    st.error(f"‚ùå Failed to load SciBERT. Install: `pip install transformers torch`")
    st.stop()

# ==============================
# EIS & ML PATTERNS
# ==============================
KEY_TERMS = [
    "electrochemical impedance spectroscopy", "EIS", "impedance data",
    "machine learning", "deep learning", "neural networks", "NLP",
    "natural language processing", "context-aware", "universal modeling",
    "state of health", "SOH", "remaining useful life", "RUL",
    "equivalent circuit model", "ECM", "distribution of relaxation times", "DRT",
    "Nyquist plot", "Bode plot", "lithium-ion battery", "transfer learning",
    "physics-informed", "transformer model", "gaussian process regression"
]

KEY_PATTERNS = [
    r'\belectrochemical impedance spectroscopy|eis\b',
    r'\bnyquist|bode\s*plots?\b',
    r'\bequivalent circuit|ecm|drt|relaxation times\b',
    r'\bmachine learning|deep learning|neural networks?|cnn|lstm|transformer\b',
    r'\buniversal modeling|transfer learning|generalizab\b',
    r'\bcontext-aware|metadata extraction|nlp|natural language\b',
    r'\bstate of health|soh|rul|remaining useful life\b',
    r'\bbattery|li-ion|lithium|lfp|nmc|fuel cell\b'
]

@st.cache_data
def compile_patterns():
    return [re.compile(pattern, re.IGNORECASE) for pattern in KEY_PATTERNS]

COMPILED_PATTERNS = compile_patterns()

# ==============================
# NLP CONTEXT EXTRACTION (NEW FEATURE)
# ==============================
def extract_context(text):
    """Specific NLP extraction for EIS Context."""
    context = {"Chemistry": "Unknown", "Temp": "Ambient", "SoC_SOH": "Not Specified"}
    
    # Extract Temperature
    temp = re.search(r'(-?\d+\s*¬∞C|-?\d+\s*K|room temperature)', text, re.I)
    if temp: context["Temp"] = temp.group(1)
    
    # Extract Chemistry
    chem = re.search(r'\b(LFP|NMC|LCO|NCA|Li-ion|Sodium|Solid-state)\b', text, re.I)
    if chem: context["Chemistry"] = chem.group(1).upper()
    
    # Extract SOH/SOC context
    health = re.search(r'\b(SOH|SOC|State of Health|State of Charge)\b', text, re.I)
    if health: context["SoC_SOH"] = health.group(1)
    
    return context

# ==============================
# SCORING & SEARCH
# ==============================
@st.cache_data
def score_abstract_with_scibert(abstract):
    try:
        inputs = scibert_tokenizer(abstract, return_tensors="pt", truncation=True, max_length=512, padding=True)
        with torch.no_grad():
            outputs = scibert_model(**inputs, output_attentions=True)
        
        num_matched = sum(1 for pat in COMPILED_PATTERNS if pat.search(abstract))
        relevance_prob = np.sqrt(num_matched) / np.sqrt(len(KEY_PATTERNS))
        
        # Attention boost for core EIS/ML terms
        tokens = scibert_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        keyword_indices = [i for i, t in enumerate(tokens) if any(kw in t.lower() for kw in ['eis', 'impedance', 'health', 'learning'])]
        if keyword_indices:
            relevance_prob = min(relevance_prob + 0.15, 1.0)
            
        return relevance_prob
    except:
        return 0.0

@st.cache_data
def query_arxiv_api(query, categories, max_results, start_year, end_year):
    client = arxiv.Client()
    search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)
    
    papers = []
    seen_ids = set()
    
    for result in client.results(search):
        if not (start_year <= result.published.year <= end_year): continue
        if not any(cat in result.categories for cat in categories): continue
        
        paper_id = result.get_short_id()
        if paper_id in seen_ids: continue
        seen_ids.add(paper_id)
        
        relevance_prob = score_abstract_with_scibert(result.summary)
        if relevance_prob < 0.3: continue
        
        ctx = extract_context(result.summary)
        
        papers.append({
            "id": paper_id,
            "title": result.title,
            "authors": ", ".join([a.name for a in result.authors]),
            "year": result.published.year,
            "categories": ", ".join(result.categories),
            "abstract": result.summary,
            "context": f"Chem: {ctx['Chemistry']} | Temp: {ctx['Temp']} | Focus: {ctx['SoC_SOH']}",
            "pdf_url": result.pdf_url,
            "relevance_prob": round(relevance_prob * 100, 2)
        })
    return papers

# ==============================
# DATABASE & FILE UTILS (KEEPING STRUCTURE)
# ==============================
def init_metadata_db():
    conn = sqlite3.connect(METADATA_DB_FILE)
    conn.execute("CREATE TABLE IF NOT EXISTS papers (id TEXT PRIMARY KEY, title TEXT, authors TEXT, year INTEGER, context TEXT, relevance_prob REAL)")
    conn.close()

# (Include your download_pdf_bytes, handle_pdf_download, inspect_metadata_db functions here - they remain identical to your previous code)
# ... [Keeping standard DB/PDF functions from your original script] ...

# ==============================
# MAIN APPLICATION LAYOUT (MATCHES YOUR IMAGE)
# ==============================
st.header("üîç arXiv Query for EIS & ML Context-Awareness")
st.markdown("Use the sidebar to configure your search. Results are scored by **SciBERT + regex relevance**.")

log_container = st.empty()
log_container.text_area("üìù Processing Logs", "\n".join(st.session_state.log_buffer), height=150)

with st.sidebar:
    st.image("https://arxiv.org/favicon.ico", width=32)
    st.subheader("‚öôÔ∏è Search Configuration")
    
    query_mode = st.radio("Query Mode", ["Auto (Recommended)", "Custom"], horizontal=True)
    if query_mode == "Auto":
        query = '("EIS" OR "Impedance Spectroscopy") AND ("Machine Learning" OR "Neural Network" OR "NLP")'
        st.text_area("Auto-generated Query", value=query, height=100, disabled=True)
    else:
        query = st.text_area("Custom Query", value="EIS AND 'Transfer Learning'", height=100)
    
    # Machine Learning and Physics Categories
    default_categories = ["stat.ML", "cs.LG", "physics.chem-ph", "eess.SP"]
    categories = st.multiselect("arXiv Categories", options=default_categories + ["physics.app-ph", "cs.AI"], default=default_categories)
    
    max_results = st.slider("Max Results", 1, 200, 30)
    start_year = st.number_input("Start Year", 2010, 2026, 2018)
    end_year = st.number_input("End Year", 2010, 2026, 2026)
    
    search_button = st.button("üöÄ Execute Search", type="primary")

if search_button:
    st.session_state.search_performed = True
    with st.spinner("üì° Analyzing EIS/ML Literature..."):
        papers = query_arxiv_api(query, categories, max_results, start_year, end_year)
        
        if not papers:
            st.warning("No papers met the criteria.")
        else:
            df = pd.DataFrame(papers)
            st.session_state.papers_df = df
            st.success(f"‚úÖ Found {len(df)} relevant papers.")
            
            for idx, paper in df.iterrows():
                with st.expander(f"üìÑ {paper['title']} ({paper['year']}) ‚Äî {paper['relevance_prob']}%"):
                    st.info(f"üß¨ **Extracted NLP Context**: {paper['context']}")
                    st.write(paper['abstract'])
                    st.markdown(f"[üåê View on arXiv]({paper['pdf_url'].replace('/pdf/', '/abs/')})")
                    # Unique key fix as requested
                    if st.button("üì• Index Full Text", key=f"idx_{paper['id']}"):
                        update_log(f"Indexing full text for {paper['id']}...")

# (Footer download buttons same as your previous code)
